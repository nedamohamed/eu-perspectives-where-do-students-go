{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors\n",
    "import json\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from sklearn.metrics import mutual_info_score, normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"\n",
    "    This function rewrites then json files so that they can be read properly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f: # Read teh file as a string\n",
    "            json_data = f.read()\n",
    "\n",
    "        corrected_json_data = json_data.replace(\"'\", '\"') # Issue being corrected\n",
    "\n",
    "        return json.loads(corrected_json_data) # return loaded data\n",
    "    \n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error loading JSON file '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "def read_edges(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read()\n",
    "        content = content.replace('\\'', '\"')\n",
    "        return json.loads(content)\n",
    "\n",
    "files = [\"data/edges-total.json\", \"data/edges-female.json\", \"data/edges-male.json\"]\n",
    "edges_total = load_json_file(files[0])\n",
    "edges_female = load_json_file(files[1])\n",
    "edges_male = load_json_file(files[2])\n",
    "nodes = load_json_file(\"data/nodes.json\")\n",
    "\n",
    "YEARS = [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "students_graphs = []\n",
    "\n",
    "# Loading data:\n",
    "\n",
    "with open('data/borders-edges.json', 'r') as f:\n",
    "    borders_edges = json.load(f)\n",
    "    \n",
    "with open('data/language-model-edges.json', 'r') as f:\n",
    "    language_edges = json.load(f)\n",
    "    \n",
    "with open('data/country-language.json', 'r') as f:\n",
    "    country_language = json.load(f)\n",
    "# Loading graphs\n",
    "\n",
    "for i in range(len(YEARS)):\n",
    "    curr_edges = edges_total[i]\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    for edge in curr_edges:\n",
    "        G.add_edge(edge['origin'], edge['destination'], weight=edge['value'])\n",
    "    students_graphs.append(G)\n",
    "\n",
    "borders_graph = nx.Graph()\n",
    "language_graph = nx.Graph()\n",
    "\n",
    "borders_graph.add_nodes_from(nodes)\n",
    "language_graph.add_nodes_from(nodes)\n",
    "\n",
    "for edge in borders_edges:\n",
    "    if edge[0] in nodes and edge[1] in nodes:\n",
    "        borders_graph.add_edge(edge[0], edge[1])\n",
    "\n",
    "for edge in language_edges:\n",
    "    language_graph.add_edge(edge['source'], edge['target'], weight=1, family=edge['family'])\n",
    "\n",
    "language_country = nx.Graph()\n",
    "\n",
    "language_country.add_nodes_from(nodes)\n",
    "\n",
    "# Add edges based on common languages\n",
    "for i in range(len(nodes)):\n",
    "    for j in range(i + 1, len(nodes)):\n",
    "        country1 = nodes[i]\n",
    "        country2 = nodes[j]\n",
    "        languages1 = set(country_language[country1])\n",
    "        languages2 = set(country_language[country2])\n",
    "        if languages1 & languages2:  # Check if there is any common language\n",
    "            language_country.add_edge(country1, country2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "num_years = len(YEARS)\n",
    "max_values_total = [max(sublist, key=lambda x: x['value'])['value'] for sublist in edges_total]\n",
    "max_values_female = [max(sublist, key=lambda x: x['value'])['value'] for sublist in edges_female]\n",
    "max_values_male = [max(sublist, key=lambda x: x['value'])['value'] for sublist in edges_male]\n",
    "\n",
    "min_values_total = [min(sublist, key=lambda x: x['value'])['value'] for sublist in edges_total]\n",
    "min_values_female = [min(sublist, key=lambda x: x['value'])['value'] for sublist in edges_female]\n",
    "min_values_male = [min(sublist, key=lambda x: x['value'])['value'] for sublist in edges_male]\n",
    "\n",
    "min_values_combined = [min(min_values_female[i], min_values_male[i]) for i in range(len(YEARS))] # When plotting males with females\n",
    "max_values_combined = [max(max_values_female[i], max_values_male[i]) for i in range(len(YEARS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_to_country = dict()\n",
    "for country, languages in country_language.items():\n",
    "    for language in languages:\n",
    "        if language not in language_to_country:\n",
    "            language_to_country[language] = []\n",
    "        language_to_country[language].append(country)\n",
    "multi_linguial_countries = {}\n",
    "for country in country_language.keys():\n",
    "    if len(country_language[country]) > 1:    \n",
    "        multi_linguial_countries[country] = country_language[country] \n",
    "nx.write_gexf(language_country, 'language_country.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_edges_total = []\n",
    "for year_data, max_val, min_val in zip(edges_total, max_values_total, min_values_total):\n",
    "    normalized_year_data = []\n",
    "    for entry in year_data:\n",
    "        normalized_value = (entry['value'] - min_val) / (max_val - min_val)\n",
    "        normalized_year_data.append({'origin': entry['origin'], 'destination': entry['destination'], 'value': normalized_value})\n",
    "    normalized_edges_total.append(normalized_year_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plotly.colors.qualitative.Plotly * (37 // len(plotly.colors.qualitative.Plotly) + 1)\n",
    "colors = colors[:37]  # Ensure the list has exactly 37 colors\n",
    "color_map = {node: colors[i % len(colors)] for i, node in enumerate(nodes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=num_years, cols=1, subplot_titles=[f'Year {year}' for year in YEARS])\n",
    "\n",
    "for i, (year, year_data) in enumerate(zip(YEARS, normalized_edges_total)):\n",
    "    # Create a dictionary to accumulate values by origin and destination\n",
    "    aggregated_data = {}\n",
    "    for entry in year_data:\n",
    "        origin = entry['origin']\n",
    "        destination = entry['destination']\n",
    "        value = entry['value']\n",
    "        if origin not in aggregated_data:\n",
    "            aggregated_data[origin] = {}\n",
    "        if destination not in aggregated_data[origin]:\n",
    "            aggregated_data[origin][destination] = 0\n",
    "        aggregated_data[origin][destination] += value\n",
    "\n",
    "    origins = list(aggregated_data.keys())\n",
    "    destinations = set(dest for orig in aggregated_data.values() for dest in orig.keys())\n",
    "\n",
    "    for destination in destinations:\n",
    "        values = [aggregated_data[orig].get(destination, 0) for orig in origins]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=origins,\n",
    "                y=values,\n",
    "                name=destination,\n",
    "                marker=dict(color=color_map[destination]),\n",
    "                showlegend=(i == 0)\n",
    "            ),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "\n",
    "fig.update_layout(height=400*num_years, title_text='Normalized Stacked Bar Charts for Each Year showing Outgoing Students', barmode='stack')\n",
    "\n",
    "fig.write_html('outgoing_students.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=num_years, cols=1, subplot_titles=[f'Year {year}' for year in YEARS])\n",
    "\n",
    "for i, (year, year_data) in enumerate(zip(YEARS, normalized_edges_total)):\n",
    "    # Create a dictionary to accumulate values by destination and origin\n",
    "    aggregated_data = {}\n",
    "    for entry in year_data:\n",
    "        origin = entry['origin']\n",
    "        destination = entry['destination']\n",
    "        value = entry['value']\n",
    "        if destination not in aggregated_data:\n",
    "            aggregated_data[destination] = {}\n",
    "        if origin not in aggregated_data[destination]:\n",
    "            aggregated_data[destination][origin] = 0\n",
    "        aggregated_data[destination][origin] += value\n",
    "\n",
    "    destinations = list(aggregated_data.keys())\n",
    "    origins = set(orig for dest in aggregated_data.values() for orig in dest.keys())\n",
    "\n",
    "    for origin in origins:\n",
    "        values = [aggregated_data[dest].get(origin, 0) for dest in destinations]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=destinations,\n",
    "                y=values,\n",
    "                name=origin,\n",
    "                marker=dict(color=color_map[origin]),\n",
    "                showlegend=(i == 0)\n",
    "            ),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "\n",
    "fig.update_layout(height=400*num_years, title_text='Normalized Stacked Bar Charts for Each Year showing Incoming Students', barmode='stack')\n",
    "\n",
    "fig.write_html('incoming_students.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_total = [dict() for _ in YEARS]\n",
    "outgoing_total = [dict() for _ in YEARS]\n",
    "\n",
    "for i, year_data in enumerate(normalized_edges_total):\n",
    "    for entry in year_data:\n",
    "        origin = entry['origin']\n",
    "        destination = entry['destination']\n",
    "        value = entry['value']\n",
    "        if origin not in outgoing_total[i]:\n",
    "            outgoing_total[i][origin] = 0\n",
    "        if destination not in incoming_total[i]:\n",
    "            incoming_total[i][destination] = 0\n",
    "        outgoing_total[i][origin] += value\n",
    "        incoming_total[i][destination] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=num_years, cols=1, subplot_titles=[f'Year {year}' for year in YEARS])\n",
    "\n",
    "for i, (incoming_data, outgoing_data) in enumerate(zip(incoming_total, outgoing_total)):\n",
    "    nodes = list(set(incoming_data.keys()).union(set(outgoing_data.keys())))\n",
    "\n",
    "    incoming_values = [incoming_data.get(node, 0) for node in nodes]\n",
    "    outgoing_values = [outgoing_data.get(node, 0) for node in nodes]\n",
    "\n",
    "    colors = {'Incoming': 'blue', 'Outgoing': 'green'}\n",
    "\n",
    "    for name, values, color in [('Outgoing', outgoing_values, colors['Outgoing']), ('Incoming', incoming_values, colors['Incoming'])]:\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=values, y=nodes, orientation='h', name=name, marker_color=color),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "\n",
    "fig.update_layout(height=400*num_years, title_text='Incoming and Outgoing Edge Weights for Each Node', barmode='stack')\n",
    "\n",
    "fig.write_html('incoming_outgoing.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_total_male = [dict() for _ in YEARS]\n",
    "outgoing_total_male = [dict() for _ in YEARS]\n",
    "incoming_total_female = [dict() for _ in YEARS]\n",
    "outgoing_total_female = [dict() for _ in YEARS]\n",
    "\n",
    "for i, year in enumerate(YEARS):\n",
    "    # Male data\n",
    "    for edge in edges_male[i]:\n",
    "        origin, destination, value = edge['origin'], edge['destination'], edge['value']\n",
    "        if destination not in incoming_total_male[i]:\n",
    "            incoming_total_male[i][destination] = 0\n",
    "        incoming_total_male[i][destination] += value\n",
    "        if origin not in outgoing_total_male[i]:\n",
    "            outgoing_total_male[i][origin] = 0\n",
    "        outgoing_total_male[i][origin] += value\n",
    "\n",
    "    # Female data\n",
    "    for edge in edges_female[i]:\n",
    "        origin, destination, value = edge['origin'], edge['destination'], edge['value']\n",
    "        if destination not in incoming_total_female[i]:\n",
    "            incoming_total_female[i][destination] = 0\n",
    "        incoming_total_female[i][destination] += value\n",
    "        if origin not in outgoing_total_female[i]:\n",
    "            outgoing_total_female[i][origin] = 0\n",
    "        outgoing_total_female[i][origin] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = make_subplots(rows=num_years, cols=1, subplot_titles=[f'Year {year}' for year in YEARS])\n",
    "\n",
    "for i, year in enumerate(YEARS):\n",
    "    # Calculate net change for males\n",
    "    male_net_change = {}\n",
    "    for node in set(incoming_total_male[i].keys()).union(set(outgoing_total_male[i].keys())):\n",
    "        incoming_value = incoming_total_male[i].get(node, 0)\n",
    "        outgoing_value = outgoing_total_male[i].get(node, 0)\n",
    "        male_net_change[node] = incoming_value - outgoing_value\n",
    "\n",
    "    # Calculate net change for females\n",
    "    female_net_change = {}\n",
    "    for node in set(incoming_total_female[i].keys()).union(set(outgoing_total_female[i].keys())):\n",
    "        incoming_value = incoming_total_female[i].get(node, 0)\n",
    "        outgoing_value = outgoing_total_female[i].get(node, 0)\n",
    "        female_net_change[node] = incoming_value - outgoing_value\n",
    "\n",
    "    # Calculate total net change\n",
    "    total_net_change = {node: male_net_change.get(node, 0) + female_net_change.get(node, 0)\n",
    "                        for node in set(male_net_change.keys()).union(set(female_net_change.keys()))}\n",
    "\n",
    "    # Normalize the total net change\n",
    "    max_net_change = max(abs(value) for value in total_net_change.values())\n",
    "    if max_net_change != 0:\n",
    "        total_net_change = {node: value / max_net_change for node, value in total_net_change.items()}\n",
    "\n",
    "    nodes = list(total_net_change.keys())\n",
    "    total_net_change_values = [total_net_change[node] for node in nodes]\n",
    "\n",
    "    # Create trace for total net change\n",
    "    fig1.add_trace(\n",
    "        go.Bar(x=nodes, y=total_net_change_values, name=f'Total Net Change {year}', marker_color='grey', showlegend=False),\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "\n",
    "fig1.update_layout(height=400*num_years, title_text='Normalized Total Net Change in Edge Weights for Each Node')\n",
    "fig1.write_html('normalized_total_net_change.html')\n",
    "\n",
    "# 2. Create the second graph: plot of subplots showing net change of males vs females over the years for each node\n",
    "\n",
    "fig2 = make_subplots(rows=len(nodes), cols=1, subplot_titles=nodes)\n",
    "\n",
    "for j, node in enumerate(nodes):\n",
    "    male_net_change_values = []\n",
    "    female_net_change_values = []\n",
    "    \n",
    "    for i, year in enumerate(YEARS):\n",
    "        # Calculate net change for the node\n",
    "        male_net_change = incoming_total_male[i].get(node, 0) - outgoing_total_male[i].get(node, 0)\n",
    "        female_net_change = incoming_total_female[i].get(node, 0) - outgoing_total_female[i].get(node, 0)\n",
    "        \n",
    "        male_net_change_values.append(male_net_change)\n",
    "        female_net_change_values.append(female_net_change)\n",
    "\n",
    "    # Add male net change trace\n",
    "    fig2.add_trace(\n",
    "        go.Bar(x=YEARS, y=male_net_change_values, name=f'Male Net Change {node}', marker_color='blue', showlegend=(j == 0)),\n",
    "        row=j+1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add female net change trace\n",
    "    fig2.add_trace(\n",
    "        go.Bar(x=YEARS, y=female_net_change_values, name=f'Female Net Change {node}', marker_color='pink', showlegend=(j == 0)),\n",
    "        row=j+1, col=1\n",
    "    )\n",
    "\n",
    "fig2.update_layout(height=400*len(nodes), title_text='Net Change in Edge Weights for Males vs Females Over the Years for Each Node')\n",
    "fig2.write_html('net_change_male_vs_female.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Network Degree Centrality: TUR \n",
      "\n",
      "Border Network Degree Centrality: TUR \n",
      "\n",
      "---------------------------------\n",
      "Student Network Degree Centrality for 2013: TUR\n",
      "---------------------------------\n",
      "Student Network Degree Centrality for 2014: TUR\n",
      "---------------------------------\n",
      "Student Network Degree Centrality for 2015: TUR\n",
      "---------------------------------\n",
      "Student Network Degree Centrality for 2016: TUR\n",
      "---------------------------------\n",
      "Student Network Degree Centrality for 2017: TUR\n",
      "---------------------------------\n",
      "Student Network Degree Centrality for 2018: TUR\n",
      "---------------------------------\n",
      "Student Network Degree Centrality for 2019: TUR\n",
      "---------------------------------\n",
      "Student Network Degree Centrality for 2020: TUR\n",
      "---------------------------------\n",
      "Student Network Degree Centrality for 2021: TUR\n",
      "---------------------------------\n",
      "Student Network Degree Centrality for 2022: TUR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jp/bdm9xg0d6870cmmvqmgc1sgw0000gn/T/ipykernel_53095/3943651817.py:3: FutureWarning:\n",
      "\n",
      "adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "\n",
      "/var/folders/jp/bdm9xg0d6870cmmvqmgc1sgw0000gn/T/ipykernel_53095/3943651817.py:4: FutureWarning:\n",
      "\n",
      "adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "\n",
      "/var/folders/jp/bdm9xg0d6870cmmvqmgc1sgw0000gn/T/ipykernel_53095/3943651817.py:5: FutureWarning:\n",
      "\n",
      "adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adjacency matrices\n",
    "\n",
    "borders_adjacency = nx.adjacency_matrix(borders_graph)\n",
    "language_adjacency = nx.adjacency_matrix(language_graph)\n",
    "students_flow_adjacency = [nx.adjacency_matrix(graph) for graph in students_graphs]\n",
    "\n",
    "deg_cent_language = nx.degree_centrality(language_graph)\n",
    "deg_cent_border = nx.degree_centrality(borders_graph)\n",
    "deg_cent_student = [nx.degree_centrality(graph) for graph in students_graphs]\n",
    "\n",
    "print(\"Language Network Degree Centrality:\", max(deg_cent_language), '\\n')\n",
    "print(\"Border Network Degree Centrality:\", max(deg_cent_border), '\\n')\n",
    "for i, year in enumerate(YEARS):\n",
    "    print('---------------------------------')\n",
    "    print(f\"Student Network Degree Centrality for {year}:\",max(deg_cent_student[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Network Closeness Centrality for 2013: {'ALB': 0.0, 'AUT': 1.0, 'BEL': 0.9736842105263158, 'BGR': 0.925, 'CHE': 1.0, 'CYP': 0.7115384615384616, 'CZE': 0.9736842105263158, 'DEU': 1.0, 'DNK': 1.0, 'EST': 0.8604651162790697, 'ESP': 0.9736842105263158, 'FIN': 1.0, 'FRA': 1.0, 'HRV': 0.7551020408163265, 'HUN': 0.9736842105263158, 'ITA': 0.9736842105263158, 'LTU': 0.8809523809523809, 'LUX': 0.9024390243902439, 'LVA': 0.9024390243902439, 'MKD': 0.7115384615384616, 'NLD': 1.0, 'NOR': 0.9736842105263158, 'POL': 0.9487179487179487, 'PRT': 0.9736842105263158, 'ROU': 0.9024390243902439, 'SRB': 0.7872340425531915, 'SWE': 1.0, 'SVN': 0.925, 'SVK': 0.9736842105263158, 'TUR': 0.9024390243902439, 'GBR': 1.0, 'GRC': 0.9487179487179487, 'ISL': 0.8809523809523809, 'LIE': 0.6727272727272727, 'MLT': 0.8222222222222222, 'BIH': 0.0, 'IRL': 0.0, 'MNE': 0.0}\n",
      "Language Network Closeness Centrality: {'ALB': 0.1589825119236884, 'AUT': 0.30405405405405406, 'BEL': 0.40540540540540543, 'BGR': 0.18018018018018017, 'CHE': 0.40540540540540543, 'CYP': 0.16891891891891891, 'CZE': 0.05405405405405406, 'DEU': 0.30405405405405406, 'DNK': 0.05405405405405406, 'EST': 0.05405405405405406, 'ESP': 0.26439482961222094, 'FIN': 0.05405405405405406, 'FRA': 0.26439482961222094, 'HRV': 0.18018018018018017, 'HUN': 0.05405405405405406, 'ITA': 0.26439482961222094, 'LTU': 0.02702702702702703, 'LUX': 0.40540540540540543, 'LVA': 0.02702702702702703, 'MKD': 0.2457002457002457, 'NLD': 0.30405405405405406, 'NOR': 0.30405405405405406, 'POL': 0.05405405405405406, 'PRT': 0.26439482961222094, 'ROU': 0.26439482961222094, 'SRB': 0.18018018018018017, 'SWE': 0.05405405405405406, 'SVN': 0.18018018018018017, 'SVK': 0.05405405405405406, 'TUR': 0.10810810810810811, 'GBR': 0.30405405405405406, 'GRC': 0.1589825119236884, 'ISL': 0.05405405405405406, 'LIE': 0.30405405405405406, 'MLT': 0.30405405405405406, 'BIH': 0.18018018018018017, 'IRL': 0.30405405405405406, 'MNE': 0.18018018018018017}\n",
      "Border Network Closeness Centrality: {'ALB': 0.2164736164736165, 'AUT': 0.3342607313195548, 'BEL': 0.2418056354226567, 'BGR': 0.21855509355509356, 'CHE': 0.2877180978446801, 'CYP': 0.0, 'CZE': 0.2877180978446801, 'DEU': 0.3156906906906907, 'DNK': 0.22729729729729728, 'EST': 0.14385904892234005, 'ESP': 0.20294401544401547, 'FIN': 0.05405405405405406, 'FRA': 0.26429918290383403, 'HRV': 0.2738521654184305, 'HUN': 0.3247104247104247, 'ITA': 0.2738521654184305, 'LTU': 0.21855509355509356, 'LUX': 0.23926031294452346, 'LVA': 0.17484407484407485, 'MKD': 0.2164736164736165, 'NLD': 0.2295932295932296, 'NOR': 0.05405405405405406, 'POL': 0.28412162162162163, 'PRT': 0.16235521235521236, 'ROU': 0.26126126126126126, 'SRB': 0.27719182597231373, 'SWE': 0.05405405405405406, 'SVN': 0.29140679140679143, 'SVK': 0.29907539118065435, 'TUR': 0.17484407484407485, 'GBR': 0.02702702702702703, 'GRC': 0.1789742498403916, 'ISL': 0.0, 'LIE': 0.24706227967097533, 'MLT': 0.0, 'BIH': 0.22067698766727892, 'IRL': 0.02702702702702703, 'MNE': 0.22504682900722503}\n"
     ]
    }
   ],
   "source": [
    "close_cent_students = [nx.closeness_centrality(graph) for graph in students_graphs]\n",
    "print(\"Student Network Closeness Centrality for 2013:\", (close_cent_students[0]))\n",
    "close_cent_language = nx.closeness_centrality(language_graph)\n",
    "print(\"Language Network Closeness Centrality:\", (close_cent_language))\n",
    "close_cent_border = nx.closeness_centrality(borders_graph)\n",
    "print(\"Border Network Closeness Centrality:\", (close_cent_border))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Closeness Centrality Node for Student Network in 2013: AUT, Closeness Centrality: 1.0\n",
      "Max Closeness Centrality Node for Student Network in 2014: AUT, Closeness Centrality: 1.0\n",
      "Max Closeness Centrality Node for Student Network in 2015: AUT, Closeness Centrality: 1.0\n",
      "Max Closeness Centrality Node for Student Network in 2016: AUT, Closeness Centrality: 1.0\n",
      "Max Closeness Centrality Node for Student Network in 2017: AUT, Closeness Centrality: 1.0\n",
      "Max Closeness Centrality Node for Student Network in 2018: AUT, Closeness Centrality: 1.0\n",
      "Max Closeness Centrality Node for Student Network in 2019: AUT, Closeness Centrality: 1.0\n",
      "Max Closeness Centrality Node for Student Network in 2020: AUT, Closeness Centrality: 1.0\n",
      "Max Closeness Centrality Node for Student Network in 2021: AUT, Closeness Centrality: 1.0\n",
      "Max Closeness Centrality Node for Student Network in 2022: AUT, Closeness Centrality: 1.0\n",
      "Max Closeness Centrality Node for Language Network: BEL, Closeness Centrality: 0.40540540540540543\n",
      "Max Closeness Centrality Node for Border Network: AUT, Closeness Centrality: 0.3342607313195548\n"
     ]
    }
   ],
   "source": [
    "for year, close_cent in zip(range(2013, 2023), close_cent_students):\n",
    "    max_node = max(close_cent, key=close_cent.get)\n",
    "    max_closeness = close_cent[max_node]\n",
    "    print(f\"Max Closeness Centrality Node for Student Network in {year}: {max_node}, Closeness Centrality: {max_closeness}\")\n",
    "\n",
    "# For language network\n",
    "max_node_lang = max(close_cent_language, key=close_cent_language.get)\n",
    "max_closeness_lang = close_cent_language[max_node_lang]\n",
    "print(f\"Max Closeness Centrality Node for Language Network: {max_node_lang}, Closeness Centrality: {max_closeness_lang}\")\n",
    "\n",
    "# For border network\n",
    "max_node_border = max(close_cent_border, key=close_cent_border.get)\n",
    "max_closeness_border = close_cent_border[max_node_border]\n",
    "print(f\"Max Closeness Centrality Node for Border Network: {max_node_border}, Closeness Centrality: {max_closeness_border}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation between Student Network and Language Network Centrality: 0.13997349973755227\n",
      "Spearman Correlation between Student Network and Border Network Centrality: 0.2984273115335929\n"
     ]
    }
   ],
   "source": [
    "average_centrality_students = {}\n",
    "num_networks = len(close_cent_students)\n",
    "\n",
    "for close_cent in close_cent_students:\n",
    "    for node, cent in close_cent.items():\n",
    "        if node not in average_centrality_students:\n",
    "            average_centrality_students[node] = 0\n",
    "        average_centrality_students[node] += cent\n",
    "\n",
    "# Average the centrality values\n",
    "for node in average_centrality_students:\n",
    "    average_centrality_students[node] /= num_networks\n",
    "\n",
    "# Extract the centrality values for nodes in the language network\n",
    "language_centrality_values = []\n",
    "student_centrality_values_lang = []\n",
    "\n",
    "for node, cent in close_cent_language.items():\n",
    "    if node in average_centrality_students:\n",
    "        language_centrality_values.append(cent)\n",
    "        student_centrality_values_lang.append(average_centrality_students[node])\n",
    "\n",
    "# Extract the centrality values for nodes in the border network\n",
    "border_centrality_values = []\n",
    "student_centrality_values_border = []\n",
    "\n",
    "for node, cent in close_cent_border.items():\n",
    "    if node in average_centrality_students:\n",
    "        border_centrality_values.append(cent)\n",
    "        student_centrality_values_border.append(average_centrality_students[node])\n",
    "\n",
    "# Calculate the Spearman correlation\n",
    "spearman_corr_lang, _ = stats.spearmanr(student_centrality_values_lang, language_centrality_values)\n",
    "spearman_corr_border, _ = stats.spearmanr(student_centrality_values_border, border_centrality_values)\n",
    "\n",
    "print(f\"Spearman Correlation between Student Network and Language Network Centrality: {spearman_corr_lang}\")\n",
    "print(f\"Spearman Correlation between Student Network and Border Network Centrality: {spearman_corr_border}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation between Student Network and Language Network Centrality: 0.13997349973755227, p-value: 0.4019360312289897\n",
      "Spearman Correlation between Student Network and Border Network Centrality: 0.2984273115335929, p-value: 0.06877474113799502\n"
     ]
    }
   ],
   "source": [
    "spearman_corr_lang, p_value_lang = stats.spearmanr(student_centrality_values_lang, language_centrality_values)\n",
    "spearman_corr_border, p_value_border = stats.spearmanr(student_centrality_values_border, border_centrality_values)\n",
    "\n",
    "print(f\"Spearman Correlation between Student Network and Language Network Centrality: {spearman_corr_lang}, p-value: {p_value_lang}\")\n",
    "print(f\"Spearman Correlation between Student Network and Border Network Centrality: {spearman_corr_border}, p-value: {p_value_border}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Degree Centrality Node for Student Network in 2013: AUT, Degree Centrality: 1.891891891891892\n",
      "Max Degree Centrality Node for Student Network in 2014: AUT, Degree Centrality: 1.918918918918919\n",
      "Max Degree Centrality Node for Student Network in 2015: AUT, Degree Centrality: 1.891891891891892\n",
      "Max Degree Centrality Node for Student Network in 2016: AUT, Degree Centrality: 1.891891891891892\n",
      "Max Degree Centrality Node for Student Network in 2017: AUT, Degree Centrality: 1.891891891891892\n",
      "Max Degree Centrality Node for Student Network in 2018: AUT, Degree Centrality: 1.891891891891892\n",
      "Max Degree Centrality Node for Student Network in 2019: AUT, Degree Centrality: 1.891891891891892\n",
      "Max Degree Centrality Node for Student Network in 2020: AUT, Degree Centrality: 1.864864864864865\n",
      "Max Degree Centrality Node for Student Network in 2021: AUT, Degree Centrality: 1.864864864864865\n",
      "Max Degree Centrality Node for Student Network in 2022: AUT, Degree Centrality: 1.864864864864865\n",
      "Max Degree Centrality Node for Language Network: BEL, Degree Centrality: 0.40540540540540543\n",
      "Max Degree Centrality Node for Border Network: DEU, Degree Centrality: 0.24324324324324326\n"
     ]
    }
   ],
   "source": [
    "def find_max_node_degree(degree_dict):\n",
    "    max_node = max(degree_dict, key=degree_dict.get)\n",
    "    return max_node, degree_dict[max_node]\n",
    "\n",
    "for year, graph in zip(range(2013, 2023), students_graphs):\n",
    "    degree_cent = nx.degree_centrality(graph)\n",
    "    max_node, max_degree = find_max_node_degree(degree_cent)\n",
    "    print(f\"Max Degree Centrality Node for Student Network in {year}: {max_node}, Degree Centrality: {max_degree}\")\n",
    "\n",
    "# For language network\n",
    "degree_cent_language = nx.degree_centrality(language_graph)\n",
    "max_node_lang, max_degree_lang = find_max_node_degree(degree_cent_language)\n",
    "print(f\"Max Degree Centrality Node for Language Network: {max_node_lang}, Degree Centrality: {max_degree_lang}\")\n",
    "\n",
    "# For border network\n",
    "degree_cent_border = nx.degree_centrality(borders_graph)\n",
    "max_node_border, max_degree_border = find_max_node_degree(degree_cent_border)\n",
    "print(f\"Max Degree Centrality Node for Border Network: {max_node_border}, Degree Centrality: {max_degree_border}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Border Network Eigenvector Centrality: TUR \n",
      "\n",
      "---------------------------------\n",
      "Student Network Eigenvector Centrality for 2013: TUR\n",
      "---------------------------------\n",
      "Student Network Eigenvector Centrality for 2014: TUR\n",
      "---------------------------------\n",
      "Student Network Eigenvector Centrality for 2015: TUR\n",
      "---------------------------------\n",
      "Student Network Eigenvector Centrality for 2016: TUR\n",
      "---------------------------------\n",
      "Student Network Eigenvector Centrality for 2017: TUR\n",
      "---------------------------------\n",
      "Student Network Eigenvector Centrality for 2018: TUR\n",
      "---------------------------------\n",
      "Student Network Eigenvector Centrality for 2019: TUR\n",
      "---------------------------------\n",
      "Student Network Eigenvector Centrality for 2020: TUR\n",
      "---------------------------------\n",
      "Student Network Eigenvector Centrality for 2021: TUR\n",
      "---------------------------------\n",
      "Student Network Eigenvector Centrality for 2022: TUR\n"
     ]
    }
   ],
   "source": [
    "# eig_cent_language = nx.eigenvector_centrality(language_graph) NOTE: NOT IMPLEMENTED FOR MULTIGRAPHS\n",
    "eig_cent_border = nx.eigenvector_centrality(borders_graph)\n",
    "eig_cent_student = [nx.eigenvector_centrality(graph) for graph in students_graphs]\n",
    "\n",
    "# print(\"Language Network Eigenvector Centrality:\", max(eig_cent_language), '\\n')\n",
    "print(\"Border Network Eigenvector Centrality:\", max(eig_cent_border), '\\n')\n",
    "for i, year in enumerate(YEARS):\n",
    "    print('---------------------------------')\n",
    "    print(f\"Student Network Eigenvector Centrality for {year}:\",max(eig_cent_student[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Network Community: frozenset({'TUR', 'MKD', 'SVN', 'SRB', 'GRC', 'BGR', 'ALB', 'BIH', 'MNE', 'CYP', 'HRV'})\n",
      "Language Network Community: frozenset({'NLD', 'AUT', 'GBR', 'IRL', 'MLT', 'NOR', 'LIE', 'DEU'})\n",
      "Language Network Community: frozenset({'PRT', 'LUX', 'CHE', 'ITA', 'FRA', 'ROU', 'BEL', 'ESP'})\n",
      "Language Network Community: frozenset({'HUN', 'EST', 'FIN'})\n",
      "Language Network Community: frozenset({'ISL', 'DNK', 'SWE'})\n",
      "Language Network Community: frozenset({'POL', 'CZE', 'SVK'})\n",
      "Language Network Community: frozenset({'LTU', 'LVA'})\n",
      "\n",
      "Border Network Community: frozenset({'NLD', 'LUX', 'CHE', 'LIE', 'SVN', 'ITA', 'DNK', 'FRA', 'AUT', 'BEL', 'DEU'})\n",
      "Border Network Community: frozenset({'HUN', 'TUR', 'MKD', 'SRB', 'GRC', 'BGR', 'ALB', 'ROU', 'BIH', 'MNE', 'HRV'})\n",
      "Border Network Community: frozenset({'LTU', 'CZE', 'LVA', 'EST', 'SVK', 'POL'})\n",
      "Border Network Community: frozenset({'NOR', 'SWE', 'FIN'})\n",
      "Border Network Community: frozenset({'ESP', 'PRT'})\n",
      "Border Network Community: frozenset({'GBR', 'IRL'})\n",
      "Border Network Community: frozenset({'CYP'})\n",
      "Border Network Community: frozenset({'ISL'})\n",
      "Border Network Community: frozenset({'MLT'})\n",
      "\n",
      "---------------------------------\n",
      "Student Network Community: frozenset({'HUN', 'TUR', 'PRT', 'CHE', 'LIE', 'DNK', 'FRA', 'IRL', 'FIN', 'ISL', 'DEU', 'BEL', 'GBR', 'MLT', 'CZE', 'NOR', 'SVK', 'LTU', 'AUT', 'GRC', 'LVA', 'EST'})\n",
      "\n",
      "Student Network Community: frozenset({'ALB', 'ESP', 'CYP', 'NLD', 'LUX', 'MKD', 'SWE', 'SVN', 'ITA', 'SRB', 'BGR', 'ROU', 'BIH', 'MNE', 'POL', 'HRV'})\n",
      "\n",
      "---------------------------------\n",
      "Student Network Community: frozenset({'HUN', 'PRT', 'CHE', 'EST', 'LIE', 'DNK', 'FRA', 'IRL', 'FIN', 'ISL', 'BEL', 'ESP', 'DEU', 'CYP', 'NLD', 'GBR', 'MLT', 'CZE', 'LUX', 'SWE', 'ITA', 'SVK', 'GRC', 'LTU', 'AUT', 'BGR', 'LVA', 'POL'})\n",
      "\n",
      "Student Network Community: frozenset({'TUR', 'MKD', 'NOR', 'SVN', 'SRB', 'ALB', 'ROU', 'BIH', 'MNE', 'HRV'})\n",
      "\n",
      "---------------------------------\n",
      "Student Network Community: frozenset({'PRT', 'CHE', 'LIE', 'DNK', 'POL', 'FRA', 'IRL', 'FIN', 'ISL', 'DEU', 'NLD', 'GBR', 'MLT', 'CZE', 'LUX', 'SWE', 'NOR', 'ITA', 'SVK', 'GRC', 'LTU', 'AUT', 'BGR', 'LVA', 'ROU', 'EST'})\n",
      "\n",
      "Student Network Community: frozenset({'HUN', 'TUR', 'MKD', 'SVN', 'SRB', 'ALB', 'BIH', 'MNE', 'ESP', 'HRV'})\n",
      "\n",
      "Student Network Community: frozenset({'BEL', 'CYP'})\n",
      "\n",
      "---------------------------------\n",
      "Student Network Community: frozenset({'HUN', 'PRT', 'CHE', 'LIE', 'DNK', 'POL', 'FRA', 'IRL', 'FIN', 'ISL', 'BEL', 'DEU', 'ESP', 'CYP', 'NLD', 'GBR', 'MLT', 'CZE', 'LUX', 'SWE', 'ITA', 'SVK', 'LTU', 'AUT', 'LVA', 'ROU', 'EST'})\n",
      "\n",
      "Student Network Community: frozenset({'TUR', 'MKD', 'NOR', 'SVN', 'GRC', 'SRB', 'BGR', 'ALB', 'BIH', 'MNE', 'HRV'})\n",
      "\n",
      "---------------------------------\n",
      "Student Network Community: frozenset({'HUN', 'TUR', 'PRT', 'CHE', 'EST', 'LIE', 'DNK', 'FRA', 'IRL', 'FIN', 'ISL', 'BEL', 'DEU', 'ESP', 'CYP', 'NLD', 'GBR', 'MLT', 'CZE', 'SWE', 'LUX', 'ITA', 'SVK', 'GRC', 'LTU', 'AUT', 'BGR', 'LVA', 'ROU', 'POL'})\n",
      "\n",
      "Student Network Community: frozenset({'MKD', 'NOR', 'SVN', 'SRB', 'ALB', 'BIH', 'MNE', 'HRV'})\n",
      "\n",
      "---------------------------------\n",
      "Student Network Community: frozenset({'TUR', 'CHE', 'LIE', 'DNK', 'FRA', 'ALB', 'ESP', 'BEL', 'DEU', 'NLD', 'GBR', 'LUX', 'SWE', 'MKD', 'SVN', 'ITA', 'GRC', 'SRB', 'AUT', 'BGR', 'ROU', 'BIH', 'MNE', 'HRV'})\n",
      "\n",
      "Student Network Community: frozenset({'HUN', 'MLT', 'CZE', 'PRT', 'EST', 'NOR', 'SVK', 'LTU', 'IRL', 'LVA', 'FIN', 'ISL', 'POL', 'CYP'})\n",
      "\n",
      "---------------------------------\n",
      "Student Network Community: frozenset({'TUR', 'CHE', 'LIE', 'DNK', 'FRA', 'ALB', 'BEL', 'DEU', 'ESP', 'NLD', 'GBR', 'LUX', 'MKD', 'SVN', 'ITA', 'SVK', 'GRC', 'SRB', 'AUT', 'BGR', 'ROU', 'BIH', 'MNE', 'HRV'})\n",
      "\n",
      "Student Network Community: frozenset({'HUN', 'MLT', 'CZE', 'PRT', 'SWE', 'EST', 'NOR', 'LTU', 'IRL', 'LVA', 'FIN', 'ISL', 'POL', 'CYP'})\n",
      "\n",
      "---------------------------------\n",
      "Student Network Community: frozenset({'TUR', 'PRT', 'EST', 'LIE', 'CYP', 'CZE', 'SWE', 'LUX', 'ITA', 'SVK', 'AUT', 'BGR', 'ROU', 'HUN', 'CHE', 'DNK', 'FRA', 'IRL', 'FIN', 'ISL', 'BEL', 'DEU', 'ESP', 'NLD', 'MLT', 'NOR', 'GRC', 'LTU', 'SRB', 'LVA', 'POL', 'HRV'})\n",
      "\n",
      "Student Network Community: frozenset({'GBR', 'ALB', 'MKD', 'BIH', 'MNE', 'SVN'})\n",
      "\n",
      "---------------------------------\n",
      "Student Network Community: frozenset({'HUN', 'PRT', 'CHE', 'EST', 'LIE', 'DNK', 'FRA', 'IRL', 'FIN', 'ISL', 'DEU', 'BEL', 'CYP', 'ESP', 'NLD', 'GBR', 'MLT', 'CZE', 'LUX', 'SWE', 'NOR', 'SVN', 'SVK', 'LTU', 'AUT', 'LVA', 'ROU', 'POL'})\n",
      "\n",
      "Student Network Community: frozenset({'TUR', 'MKD', 'ITA', 'GRC', 'SRB', 'BGR', 'ALB', 'BIH', 'MNE', 'HRV'})\n",
      "\n",
      "---------------------------------\n",
      "Student Network Community: frozenset({'HUN', 'PRT', 'CHE', 'LIE', 'DNK', 'POL', 'FRA', 'IRL', 'FIN', 'ISL', 'BEL', 'DEU', 'ESP', 'CYP', 'NLD', 'MLT', 'CZE', 'LUX', 'SWE', 'NOR', 'SVN', 'ITA', 'SVK', 'LTU', 'AUT', 'LVA', 'ROU', 'EST', 'HRV'})\n",
      "\n",
      "Student Network Community: frozenset({'GBR', 'TUR', 'MKD', 'GRC', 'SRB', 'BGR', 'ALB', 'BIH', 'MNE'})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "communities_language = list(greedy_modularity_communities(language_graph))\n",
    "communities_border = list(greedy_modularity_communities(borders_graph))\n",
    "communities_student = [list(greedy_modularity_communities(graph)) for graph in students_graphs]\n",
    "\n",
    "for community in communities_language:\n",
    "    print(\"Language Network Community:\", community)\n",
    "\n",
    "print()\n",
    "\n",
    "for community in communities_border:\n",
    "    print(\"Border Network Community:\", community)\n",
    "\n",
    "print()\n",
    "\n",
    "for i, year in enumerate(YEARS):\n",
    "    print('---------------------------------')\n",
    "    for community in communities_student[i]:\n",
    "        print('Student Network Community:', community)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Overlap between Language and Border Networks: 0.2413793103448276\n"
     ]
    }
   ],
   "source": [
    "def edge_overlap(graph1, graph2):\n",
    "    edges1 = set(graph1.edges())\n",
    "    edges2 = set(graph2.edges())\n",
    "    common_edges = edges1.intersection(edges2)\n",
    "    total_edges = edges1.union(edges2)\n",
    "    return len(common_edges) / len(total_edges) if total_edges else 0\n",
    "\n",
    "overlap_language_border = edge_overlap(language_graph, borders_graph)\n",
    "print(\"Edge Overlap between Language and Border Networks:\", overlap_language_border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centrality Similarity between Language and Border Networks: 0.17898371132118096\n"
     ]
    }
   ],
   "source": [
    "def node_centrality_similarity(graph1, graph2, centrality_func):\n",
    "    centrality1 = centrality_func(graph1)\n",
    "    centrality2 = centrality_func(graph2)\n",
    "\n",
    "    common_nodes = set(centrality1.keys()).intersection(set(centrality2.keys()))\n",
    "\n",
    "    if not common_nodes:\n",
    "        return 0\n",
    "\n",
    "    vector1 = np.array([centrality1[node] for node in common_nodes])\n",
    "    vector2 = np.array([centrality2[node] for node in common_nodes])\n",
    "\n",
    "    correlation = np.corrcoef(vector1, vector2)[0, 1]\n",
    "    return correlation\n",
    "\n",
    "centrality_similarity = node_centrality_similarity(language_graph, borders_graph, nx.degree_centrality)\n",
    "print(\"Centrality Similarity between Language and Border Networks:\", centrality_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information between Language and Border Networks: 0.7826084691003484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nedaelewa/anaconda3/lib/python3.10/site-packages/sklearn/metrics/cluster/_supervised.py:64: UserWarning:\n",
      "\n",
      "Clustering metrics expects discrete values but received continuous values for label, and continuous values for target\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mutual_information(graph1, graph2):\n",
    "    centrality1 = nx.degree_centrality(graph1)\n",
    "    centrality2 = nx.degree_centrality(graph2)\n",
    "\n",
    "    # Get common nodes\n",
    "    common_nodes = set(centrality1.keys()).intersection(set(centrality2.keys()))\n",
    "\n",
    "    if not common_nodes:\n",
    "        return 0\n",
    "\n",
    "    vector1 = [centrality1[node] for node in common_nodes]\n",
    "    vector2 = [centrality2[node] for node in common_nodes]\n",
    "\n",
    "    return mutual_info_score(vector1, vector2)\n",
    "\n",
    "mutual_info = mutual_information(language_graph, borders_graph)\n",
    "print(\"Mutual Information between Language and Border Networks:\", mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directed Edge Overlap between Student Flow and Language Networks: 0.1683760683760684\n"
     ]
    }
   ],
   "source": [
    "def directed_edge_overlap(directed_graph, undirected_graph):\n",
    "    directed_edges = set(directed_graph.edges())\n",
    "    undirected_edges = set()\n",
    "    for u, v in undirected_graph.edges():\n",
    "        undirected_edges.add((u, v))\n",
    "        undirected_edges.add((v, u))  # Add both directions for undirected edge\n",
    "\n",
    "    common_edges = directed_edges.intersection(undirected_edges)\n",
    "    total_edges = directed_edges.union(undirected_edges)\n",
    "    return len(common_edges) / len(total_edges) if total_edges else 0\n",
    "\n",
    "overlap = directed_edge_overlap(students_graphs[0], language_graph)\n",
    "print(\"Directed Edge Overlap between Student Flow and Language Networks:\", overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Mutual Information (NMI) between Language and Student Network Communities: 0.005845263561664977\n"
     ]
    }
   ],
   "source": [
    "node_to_index = {node: idx for idx, node in enumerate(nodes)}\n",
    "\n",
    "def community_to_labels(communities, num_nodes):\n",
    "    labels = [-1] * num_nodes\n",
    "    for label, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            labels[node_to_index[node]] = label\n",
    "            return labels\n",
    "\n",
    "num_nodes = max(len(language_graph.nodes()), len(students_graphs[0].nodes()))\n",
    "\n",
    "language_labels = community_to_labels(communities_language, num_nodes)\n",
    "student_labels = community_to_labels(communities_student[0], num_nodes)\n",
    "\n",
    "nmi_score = normalized_mutual_info_score(language_labels, student_labels)\n",
    "print(\"Normalized Mutual Information (NMI) between Language and Student Network Communities:\", nmi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Albanian, Countries: ['ALB', 'MKD']\n",
      "Language: German, Countries: ['AUT', 'BEL', 'CHE', 'DEU', 'LUX', 'LIE']\n",
      "Language: Dutch, Countries: ['BEL', 'NLD']\n",
      "Language: French, Countries: ['BEL', 'CHE', 'FRA', 'LUX']\n",
      "Language: Italian, Countries: ['CHE', 'ITA']\n",
      "Language: Greek, Countries: ['CYP', 'GRC']\n",
      "Language: Turkish, Countries: ['CYP', 'TUR']\n",
      "Language: Croatian, Countries: ['HRV', 'BIH']\n",
      "Language: Serbian, Countries: ['SRB', 'BIH']\n",
      "Language: English, Countries: ['GBR', 'MLT', 'IRL']\n"
     ]
    }
   ],
   "source": [
    "family_country_map = dict()\n",
    "\n",
    "for edge in language_edges:\n",
    "    if edge['family'] not in family_country_map:\n",
    "        family_country_map[edge['family']] = []\n",
    "    if edge['source'] not in family_country_map[edge['family']]:\n",
    "        family_country_map[edge['family']].append(edge['source'])\n",
    "    if edge['target'] not in family_country_map[edge['family']]:\n",
    "        family_country_map[edge['family']].append(edge['target'])\n",
    "\n",
    "for language, countries in language_to_country.items():\n",
    "    if len(countries) > 1:\n",
    "        print(f\"Language: {language}, Countries: {countries}\")\n",
    "\n",
    "def separate_edges(edges, countries):\n",
    "    edges_intra =  [[] for i in range(len(YEARS))]\n",
    "    edges_inter = [[] for i in range(len(YEARS))]\n",
    "    \n",
    "    for i in range(len(YEARS)):\n",
    "        for edge in edges[i]:\n",
    "            if edge['origin'] in countries and edge['destination'] in countries:\n",
    "                edges_intra[i].append(edge)\n",
    "            elif (edge['origin'] not in countries) and (edge['destination'] in countries):\n",
    "                edges_inter[i].append(edge)\n",
    "    \n",
    "    return edges_intra, edges_inter\n",
    "\n",
    "indo_european_speaking_intra, indo_european_speaking_inter = separate_edges(edges_total, family_country_map['Indo-European'])\n",
    "nordic_intra, nordic_inter = separate_edges(edges_total, ['DNK', 'FIN', 'ISL', 'NOR', 'SWE'])\n",
    "south_slavic_speaking_intra, south_slavic_speaking_inter = separate_edges(edges_total, family_country_map['South Slavik'])\n",
    "turkic_speaking_intra, turkic_speaking_inter = separate_edges(edges_total, family_country_map['Turkic'])\n",
    "west_germanic_speaking_intra, west_germanic_speaking_inter = separate_edges(edges_total, family_country_map['West Germanic'])\n",
    "romance_speaking_intra, romance_speaking_inter = separate_edges(edges_total, family_country_map['Romance'])\n",
    "west_slavic_speaking_intra, west_slavic_speaking_inter = separate_edges(edges_total, family_country_map['West Slavic'])\n",
    "north_germanic_speaking_intra, north_germanic_speaking_inter = separate_edges(edges_total, family_country_map['North Germanic'])\n",
    "uralic_speaking_intra, uralic_speaking_inter = separate_edges(edges_total, family_country_map['Uralic'])\n",
    "baltic_speaking_intra, baltic_speaking_inter = separate_edges(edges_total, family_country_map['Baltic'])\n",
    "albanian_speaking_intra, albanian_speaking_inter = separate_edges(edges_total, language_to_country['Albanian'])\n",
    "german_speaking_intra, german_speaking_inter = separate_edges(edges_total, language_to_country['German'])\n",
    "dutch_speaking_intra, dutch_speaking_inter = separate_edges(edges_total, language_to_country['Dutch'])\n",
    "french_speaking_intra, french_speaking_inter = separate_edges(edges_total, language_to_country['French'])\n",
    "italian_speaking_intra, italian_speaking_inter = separate_edges(edges_total, language_to_country['Italian'])\n",
    "greek_speaking_intra, greek_speaking_inter = separate_edges(edges_total, language_to_country['Greek'])\n",
    "turkish_speaking_intra, turkish_speaking_inter = separate_edges(edges_total, language_to_country['Turkish'])\n",
    "croatian_speaking_intra, croatian_speaking_inter = separate_edges(edges_total, language_to_country['Croatian'])\n",
    "serbian_speaking_intra, serbian_speaking_inter = separate_edges(edges_total, language_to_country['Serbian'])\n",
    "english_speaking_intra, english_speaking_inter = separate_edges(edges_total, language_to_country['English'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportions(inter, intra, countires):\n",
    "    # structure of the tuple (same language students, different language students)\n",
    "    proportions = {country : (0, 0) for country in countires} # Country is destination\n",
    "    \n",
    "    for edge in intra:\n",
    "        value = edge['value']\n",
    "        proportions[edge['destination']] =  (proportions[edge['destination']][0] + value, proportions[edge['destination']][1])\n",
    "    \n",
    "    for edge in inter:\n",
    "        value = edge['value']\n",
    "        proportions[edge['destination']] = (proportions[edge['destination']][0] , proportions[edge['destination']][1] + value)\n",
    "    \n",
    "    return proportions\n",
    "\n",
    "german_proportions = [get_proportions(german_speaking_inter[i], german_speaking_intra[i], language_to_country['German']) for i in range(len(YEARS))]\n",
    "nordic_proportions = [get_proportions(nordic_inter[i], nordic_intra[i], ['DNK', 'FIN', 'ISL', 'NOR', 'SWE']) for i in range(len(YEARS))]\n",
    "dutch_proportions = [get_proportions(dutch_speaking_inter[i], dutch_speaking_intra[i], language_to_country['Dutch']) for i in range(len(YEARS))]\n",
    "french_proportions = [get_proportions(french_speaking_inter[i], french_speaking_intra[i], language_to_country['French']) for i in range(len(YEARS))]\n",
    "italian_proportions = [get_proportions(italian_speaking_inter[i], italian_speaking_intra[i], language_to_country['Italian']) for i in range(len(YEARS))]\n",
    "greek_proportions = [get_proportions(greek_speaking_inter[i], greek_speaking_intra[i], language_to_country['Greek']) for i in range(len(YEARS))]\n",
    "turkish_proportions = [get_proportions(turkish_speaking_inter[i], turkish_speaking_intra[i], language_to_country['Turkish']) for i in range(len(YEARS))]\n",
    "croatian_proportions = [get_proportions(croatian_speaking_inter[i], croatian_speaking_intra[i], language_to_country['Croatian']) for i in range(len(YEARS))]\n",
    "serbian_proportions = [get_proportions(serbian_speaking_inter[i], serbian_speaking_intra[i], language_to_country['Serbian']) for i in range(len(YEARS))]\n",
    "english_proportions = [get_proportions(english_speaking_inter[i], english_speaking_intra[i], language_to_country['English']) for i in range(len(YEARS))]\n",
    "indo_european_proportions = [get_proportions(indo_european_speaking_inter[i], indo_european_speaking_intra[i], family_country_map['Indo-European']) for i in range(len(YEARS))]\n",
    "south_slavic_proportions = [get_proportions(south_slavic_speaking_inter[i], south_slavic_speaking_intra[i], family_country_map['South Slavik']) for i in range(len(YEARS))]\n",
    "turkic_proportions = [get_proportions(turkic_speaking_inter[i], turkic_speaking_intra[i], family_country_map['Turkic']) for i in range(len(YEARS))]\n",
    "west_germanic_proportions = [get_proportions(west_germanic_speaking_inter[i], west_germanic_speaking_intra[i], family_country_map['West Germanic']) for i in range(len(YEARS))]\n",
    "romance_proportions = [get_proportions(romance_speaking_inter[i], romance_speaking_intra[i], family_country_map['Romance']) for i in range(len(YEARS))]\n",
    "west_slavic_proportions = [get_proportions(west_slavic_speaking_inter[i], west_slavic_speaking_intra[i], family_country_map['West Slavic']) for i in range(len(YEARS))]\n",
    "north_germanic_proportions = [get_proportions(north_germanic_speaking_inter[i], north_germanic_speaking_intra[i], family_country_map['North Germanic']) for i in range(len(YEARS))]\n",
    "uralic_proportions = [get_proportions(uralic_speaking_inter[i], uralic_speaking_intra[i], family_country_map['Uralic']) for i in range(len(YEARS))]\n",
    "baltic_proportions = [get_proportions(baltic_speaking_inter[i], baltic_speaking_intra[i], family_country_map['Baltic']) for i in range(len(YEARS))]\n",
    "albanian_proportions = [get_proportions(albanian_speaking_inter[i], albanian_speaking_intra[i], language_to_country['Albanian']) for i in range(len(YEARS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(85780, 4555), (51448, 5301), (119484, 90729), (52547, 4707), (51972, 3544), (44354, 1133), (49746, 4027), (54847, 5074), (56840, 5486), (58560, 6110)]\n",
      "[(111932, 15264), (58198, 12062), (59628, 12446), (60668, 12958), (61682, 13256), (62420, 13602), (63752, 13704), (66038, 14052), (69670, 14796), (72774, 15764)]\n",
      "[(8960, -305), (4524, 177), (4706, 335), (4687, 459), (4765, 419), (4340, 685), (4290, 693), (4290, 814), (4372, 970), (4359, 1096)]\n"
     ]
    }
   ],
   "source": [
    "belgium_proportions = [(0,0) for _ in range(10)]\n",
    "swiss_proportions = [(0,0) for _ in range(10)]\n",
    "lux_proportions = [(0,0) for _ in range(10)]\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    if i != 2:\n",
    "        bel_intra = french_proportions[i]['BEL'][0] + german_proportions[i]['BEL'][0] + dutch_proportions[i]['BEL'][0]\n",
    "        bel_inter = (french_proportions[i]['BEL'][0] + french_proportions[i]['BEL'][1]) - bel_intra # total students - intra\n",
    "\n",
    "    if i == 2:\n",
    "        bel_intra = french_proportions[i]['BEL'][1] + german_proportions[i]['BEL'][1] + dutch_proportions[i]['BEL'][1]\n",
    "        bel_inter = (french_proportions[i]['BEL'][1] + french_proportions[i]['BEL'][0]) - bel_intra # total students - intra\n",
    "\n",
    "    swiss_intra = french_proportions[i]['CHE'][0] + german_proportions[i]['CHE'][0] + italian_proportions[i]['CHE'][0]\n",
    "    swiss_inter = (french_proportions[i]['CHE'][0] + french_proportions[i]['CHE'][1]) - swiss_intra\n",
    "\n",
    "    lux_intra = french_proportions[i]['LUX'][0] + german_proportions[i]['LUX'][0]\n",
    "    lux_inter = (french_proportions[i]['LUX'][0] + french_proportions[i]['LUX'][1]) - lux_intra\n",
    "\n",
    "    bih_intra = serbian_proportions[i]['BIH'][0] + croatian_proportions[i]['BIH'][0]\n",
    "    bih_inter = (serbian_proportions[i]['BIH'][0] + serbian_proportions[i]['BIH'][1]) - bih_intra\n",
    "\n",
    "    belgium_proportions[i] = (bel_intra, bel_inter)\n",
    "    swiss_proportions[i] = (swiss_intra, swiss_inter)\n",
    "    lux_proportions[i] = (lux_intra, lux_inter)\n",
    "\n",
    "print(belgium_proportions)\n",
    "print(swiss_proportions)\n",
    "print(lux_proportions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
